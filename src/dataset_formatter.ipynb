{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target is to pair the anchorable events following the rules mentioned in TB-Dense, that is we will make pair of events in every sentence and the events in the immediate next sentence. \n",
    "\n",
    "We will follow the following format to store the dataset\n",
    "\n",
    "docid:document_d Sentence1 \\tab Sentence2 s1:tokenNo token1 s2:tokenNo token2 train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    " \n",
    "\n",
    "class eventInfo:\n",
    "    def __init__(self, token, location, sentenceid, sentence):\n",
    "        self.token = token\n",
    "        self.sentenceid = sentenceid\n",
    "        self.tokenid = location\n",
    "        self.sentence = sentence\n",
    "\n",
    "def format_dataset(path, language):\n",
    "    #path : str indicating path to the dataset\n",
    "    #language: str indicating or 'it' or 'fr' \n",
    "    df = pd.read_csv(path)\n",
    "    #keys to consider: 'docid', 'sentid', 'before', 'verb', 'after', split \n",
    "\n",
    "    f_name_event_pairs =\"../dataset/formatted_data/\"+ language + \"_event_pairs_with_tab.txt\"\n",
    "    file_event_pairs = open(f_name_event_pairs, \"a\")\n",
    "\n",
    "    i=0\n",
    "    while (i<len(df)):  ### LOOPING UNTIL THE LAST ROW\n",
    "        doc_id = df['docid'][i]\n",
    "        split = df['split'][i]\n",
    "        temp_event_list = [] #--- list of events per doc\n",
    "        while(doc_id==df['docid'][i] and i<len(df) ):  ### ---LOOPING UNTIL SAME DOC ID---\n",
    "            sentence_id = df['sentid'][i]\n",
    "            sentence = str(df['before'][i]) + \" \" + str(df['verb'][i])  +\" \" + str(df['after'][i]) \n",
    "            while(sentence_id==df['sentid'][i] and i<len(df)): ### LOOPING UNTIL SAME SENTENCE ID\n",
    "                before = str(df['before'][i])\n",
    "                token_id = len(before.split(\" \"))\n",
    "                temp_event_list.append( eventInfo( df['verb'][i], token_id , df['sentid'][i], sentence ) )\n",
    "                i+=1\n",
    "                if(i>=len(df)):\n",
    "                    break\n",
    "            if(i>=len(df)):\n",
    "                break\n",
    "        \n",
    "        temp_event_list = sorted(temp_event_list, key= lambda x:(x.sentenceid, x.tokenid))\n",
    "\n",
    "        j= 0\n",
    "        while (j<len(temp_event_list)):\n",
    "            tokenid_1 = temp_event_list[j].tokenid\n",
    "            sent_1 = temp_event_list[j].sentence\n",
    "            token_1 = temp_event_list[j].token\n",
    "            k=j+1\n",
    "            while(k<len(temp_event_list)):\n",
    "                if(temp_event_list[k].sentenceid>(temp_event_list[j].sentenceid+1)):\n",
    "                    break \n",
    "                tokenid_2 = temp_event_list[k].tokenid\n",
    "                sent_2 = temp_event_list[k].sentence\n",
    "                token_2 = temp_event_list[k].token\n",
    "                file_event_pairs.write(\"docid:\"+doc_id + \"\\t\" + sent_1 + \"\\t\" + sent_2  + \"\\t\" +  \"s1:\" + str(tokenid_1) + \"\\t\" + token_1 + \"\\t\" +  \"s2:\" + str(tokenid_2) + \"\\t\" + token_2 + \" \" +split +\"\\n\")\n",
    "                k += 1\n",
    "            j += 1    \n",
    "    file_event_pairs.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_dataPath = \"../dataset/processed_files/it_processed.csv\"\n",
    "format_dataset(it_dataPath, \"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dataPath = \"../dataset/processed_files/fr_processed.csv\"\n",
    "format_dataset(fr_dataPath, \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dataPath = \"../dataset/processed_files/es_processed.csv\"\n",
    "format_dataset(fr_dataPath, \"es\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
